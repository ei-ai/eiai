<!-- Template for PROJECT REPORT of CapstoneDesign 2025-2H, initially written by khyoo -->
<!-- 본 파일은 2025년도 컴공 졸업프로젝트의 <1차보고서> 작성을 위한 기본 양식입니다. -->
<!-- 아래에 "*"..."*" 표시는 italic체로 출력하기 위해서 사용한 것입니다. -->
<!-- "내용"에 해당하는 부분을 지우고, 여러분 과제의 내용을 작성해 주세요. -->
<!-- 이미지는 절대경로로 기입 -->

# Team-Info
| (1) 과제명 | __THANOS: Integrating HPO and NAS for Hardware-Aware Transformer Optimization under Memory Size Constraints__
|:---  |---  |
| (2) 팀 번호 / 팀 이름 | 8팀 / eiai |
| (3) 팀 구성원 | __- 김민서 (2276046)__: 리더, AI, 선행연구 서칭, 연구 환경 세팅; 소프트웨어: 운영체제 설정, 협업 툴 관리 및 지도, 오픈소스 등의 소프트웨어 버전 관리 등; 하드웨어: NPU 장비 관리 및 설치, 실험에 필요한 파서 구현, 연구주제 탐색 및 논문 서칭, 기술 테스트 및 검증, 가설과 문제 제기 및 이에 따른 구현과 실험 진행, 논문 및 보고서 작성 <br> __- 김수현 (2276067)__: 팀원, AI 관련 연구팀 팀원, 선행 논문 읽고 코드 분석, 선행 연구 코드의 수정 및 보완을 진행, 방법론 관련 논문 서칭, 보고서 작성 <br> __- 하지연 (2271063)__ : 팀원, AI 연구 팀원, 선행 연구 논문 리딩, 코드 분석, 아이디어를 위한 논문 서칭 및 구현, 보고서 작성|
| (4) 팀 지도교수 | 심재형 교수님 |
| (5) 과제 분류 | 연구 과제 |
| (6) 과제 키워드 | Hardware aware NAS, HPO, Efficient Transformer |
| (7) 과제 내용 요약 |&nbsp; Transformer 모델은 NLP 분야에서 널리 사용되고 있지만, 모델의 크기와 연산량이 증가하면서 높은 컴퓨팅 자원을 요구하게 되었다. 특히 엣지 디바이스와 같은 제한된 환경에서는 모델의 추론에서의 지연과 메모리 사용에서의 제약과 같은 문제가 발생한다. 이러한 문제를 해결하기 위해, 하드웨어 제약을 고려한 경량화 모델 탐색이 필요하다. 본 논문은 NAS(Neural Architecture Search)와 HPO(Hyperparameter Optimization)를 결합한 기법을 통해 이를 해결하고자 한다. 즉, NAS를 통해 구조를 탐색하고, HPO를 통해 성능과 제약 조건 모두를 고려하는 최적의 모델을 찾는다. 특히 NAS의 거대한 서치 스페이스가 초래하는 서치 연산 문제와 메모리 제약조건에 대한 비인지성 문제를 HPO로 보완한다. 이를 통해 주어진 하드웨어에 최적화된 효율적인 트랜스포머 모델을 설계할 수 있다. |

<br>

# Project-Summary
| 항목 | 내용 |
|:---  |---  |
| (1) 문제 정의 | &nbsp;현재 트랜스포머를 비롯한 딥러닝 기반 NLP 인공지능 기술은 우리 일상 속에 깊이 자리 잡고 있다. 그러나 이러한 서비스 대부분은 데이터 센터에서 수많은 반도체를 활용하는 공급자 중심의 클라우드 기반 AI 형태로 제공되고 있다. 그리고 클라우드 기반 AI는 대표적으로 중앙 데이터 센터와의 송수신 과정에서 발생하는 응답지연, 데이터 센터에 사용되는 막대한 유지비용, 보안에서의 취약성과 같은 문제들을 동반한다. <br/> &nbsp;이를 해결하기 위한 대안으로 중앙 클라우드에 대한 의존을 줄이는 엣지 AI가 주목받고 있지만, 현재 딥러닝 모델의 크기와 연산량이 지속적으로 증가하면서 엣지 디바이스의 제한적인 환경에서 모델의 활용에 어려움이 생기고 있다.<br/> &nbsp;이에 본 연구는 대규모 NLP 인공지능을 사용하는 기업이나 엣지 디바이스에서 LLM을 사용하고자 하는 일반 유저를 타겟 고객으로 설정하고, 엣지 디바이스가 가지는 연산능력과 메모리 등 하드웨어 환경적 제약을 고려한 모델 최적화를 실현하고자 한다.|
| (2) 기존연구와의 비교 | &nbsp; __1. 전통적인 방식의 인공지능 모델 설계__ <br/>기존의 전통적인 모델 설계 방식은 전문가의 직관과 경험을 기반으로 하이퍼파라미터나 모델 구조를 수동으로 조정해 성능을 점진적으로 향상시키는 방식이다. 이 방식은 설계자 중심의 직관적 이해와 수정에 용이하다는 장점이 있지만, 트랜스포머와 같은 복잡한 모델에는 적용이 어렵고, 시간과 비용이 많이 드는 단점이 있다. 이를 극복하기 위해 NAS(Neural Architecture Search) 기법이 등장하였고, 다양한 데이터셋과 목적에 맞춰 모델을 자동으로 설계할 수 있게 되었다.<br/><br/>&nbsp; __2. NAS (Neural Architecture Search)__ <br/> NAS는 설계자의 개입 없이 알고리즘적으로 모델 구조를 탐색하며, 복잡한 모델의 설계에서 인간의 직관과 경험에 대한 의존도를 줄여준다. 그러나, 기존의 NAS는 주로 논리적 성능 추론방식인 FLOPs에 기반한 latency 측정에 의존하고 있으며, 서치 속도의 감소와 모델의 정확도 향상에만 집중해왔다. FLOPs에 의존하지 않는 연구들은 주로 동질적인 하드웨어 환경에서만 실험을 거쳐, heterogeneous한 하드웨어 환경을 고려하지 못한다는 한계가 있다. 이는 NAS에서 탐색한 모델의 성능이 실제 하드웨어에서의 성능과 괴리가 생기게 만든다.<br/><br/>&nbsp; __3. HAT (Hardware-aware Transformer)__ <br/> HAT는 하드웨어 환경(CPU, GPU)을 고려하여 트랜스포머 모델을 자동 설계하는 NAS 기반 기법을 제안한 연구로, 하드웨어의 지연시간을 직접 인퍼런스함으로써 실제 하드웨어의 지연시간에 맞는 모델을 설계했다는 데 의의가 있다. 그러나 (1) latency를 중점적으로 탐색해 메모리 제약을 포함한 하드웨어 제약 전반을 충분히 반영하지 못했고, (2) 이에 따라 메모리 사이즈 제약조건이 강한 하드웨어에서 이루어지기 어렵다.<br/><br/>&nbsp; __4.	본 연구의 차별점 및 강점__ <br/>  본 연구는 기존의 한계를 극복하기 위해 다음과 같이 차별화된 방식을 제안한다:<br/>- FLOPs가 아닌 실제 하드웨어에서의 인퍼런스 실행을 통해 latency를 직접 측정한다.<br/>-	기존 NAS/HAT 연구들이 무시했던 메모리 제약조건을 서치 스페이스에 포함하여 엣지 디바이스 환경에 적합한 실질적 최적화를 실현한다.<br/>-	CPU, GPU뿐 아니라 NPU 환경을 고려한 연구를 진행한다.|
| (3) 제안 내용 | &nbsp;본 연구는 하드웨어 제약 환경에서의 트랜스포머 모델 효율성 문제를 해결하기 위해, NAS(Neural Architecture Search)와 HPO(Hyperparameter Optimization)를 결합한 자동화된 최적화 기법을 제안한다. <br/> &nbsp; 먼저, FLOPs 기반의 이론적 성능 추정이 실제 지연시간(latency)을 정확히 반영하지 못한다는 점을 고려하여, 실제 하드웨어에서 직접 인퍼런스를 수행하는 NAS 구조를 설계하였다. 이를 통해 각 하드웨어 환경에 맞춘 정확한 성능 측정 및 평가가 가능하도록 하였다. <br/>&nbsp; 그러나, 특수 프로세서 환경에서는 하드웨어 메모리 제약으로 인해 NAS 탐색 과정에서 모델 인퍼런스 실행이 불가능해지는 문제가 발생했다. 이에 대한 해결책으로 HPO 기법을 도입하여, NAS의 방대한 서치 공간에서 발생하는 탐색 효율 저하 문제와 메모리 비인지성 문제를 동시에 해결하고자 한다. <br/>&nbsp; 또한, 기존 연구들이 주로 CPU나 GPU 환경에 한정되어 있던 것과 달리, 본 연구에서는 NPU와 같은 AI 전용 프로세서 환경까지 고려할 수 있도록 모델 변환(model converting) 단계를 추가하였다.<br/>&nbsp; 이러한 일련의 접근을 통해, 본 연구는 성능 저하 없이 모델 크기를 줄이고, 추론 속도를 향상시키는 하드웨어 친화적 트랜스포머 모델 탐색을 실현하고자 한다. |
| (4) 기대효과 및 의의 | &nbsp;본 연구를 통해 개발된 NPU 구조에 최적화되고 경량화된 트랜스포머 모델은, 연산 자원이 제한된 엣지 디바이스 환경에서도 딥러닝 모델이 효율적으로 동작할 수 있도록 지원한다. 이를 통해 기존 클라우드 기반 AI가 안고 있던 과도한 에너지 소비, 보안 취약성, 높은 운영 비용 등의 문제를 완화하고, 보다 경제적이고 안전한 AI 서비스 운영이 가능해질 것으로 기대된다.<br/>&nbsp; 또한, 본 연구에서 제안한 NAS+HPO 기반 최적화 기법은 특정 모델에 국한되지 않고, 다른 딥러닝 모델의 최적화에도 확장 적용될 수 있는 범용적인 프레임워크로서의 가능성을 가진다. 더 나아가, NPU뿐만 아니라 다양한 하드웨어 환경에서도 적용 가능성이 열려 있어, 엣지 AI의 산업적 활용 범위를 넓히는 데 기여할 수 있을 것이다. |
| (5) 주요 기능 리스트 |&nbsp; __1. Hardware aware NAS 프레임워크__ <br/> 모델 구조를 자동으로 탐색하는 NAS 기법을 기반으로 하되, 하드웨어 환경의 제약(지연시간, 메모리 크기 등)을 고려할 수 있도록 프레임워크를 설계한다. <br/> 구성 요소는 다음과 같다:<br/> - 하드웨어 환경적 제약을 반영한 탐색 공간(Search Space) 정의<br/>-	탐색 공간에서 최적의 아키텍처를 탐색하는 (Search Strategy) 수립 <br/>-	각 아키텍처의 성능 평가(Performance Estimator)<br/><br/>&nbsp; __2. 실제 하드웨어에서의 인퍼런스 기반 성능 평가__ <br/>기존 NAS가 FLOPs로 latency를 추정하는 것과 달리, 실제 하드웨어(NPU 등)에서 직접 인퍼런스를 실행해 latency를 측정한다. 이를 기반으로 latency dataset을 생성하고, latency 제약조건을 만족하는 트랜스포머 모델의 구조를 탐색한다.<br/><br/>&nbsp; __3. 메모리 제약 조건을 반영한 탐색 공간 구현__ <br/> HAT의 한계였던 메모리 제약 미반영 문제를 해결하기 위해, 탐색 공간 내에서 메모리 제약 조건을 직접 반영한다. 이를 위해 트랜스포머의 주요 구성요소인 QKV 차원(qkv dim)을 조정 가능한 변수로 설정한다. QKV dim은 self-attention 연산에서 메모리 사용량에 직접적인 영향을 미치기 때문에, 이를 축소하면 모델 전체의 메모리 사용량을 효과적으로 줄일 수 있다. 탐색 공간에서 메모리 제약 조건의 설정은 사전 정의된 하드웨어 메모리 한도 이내의 아키텍처만 탐색 대상으로 포함하게 한다. 이로써 탐색 효율성과 실행 가능성을 모두 확보할 수 있다.<br/><br/>&nbsp; __4. 다양한 하드웨어 환경(NPU 포함) 호환을 위한 모델 변환 기능__ <br/> 훈련을 진행할 Supertransformer가 실제 다양한 하드웨어 환경에서 실행 가능하도록 모델 변환 과정을 수행한다. 구체적으로 PyTorch → ONNX 변환 흐름을 따르며, 이는 특히 NPU에서의 실행을 지원하기 위함이다. <br/> 먼저, Pytorch 로 구현된 supertransformer 모델을 ONNX 형식으로 변환함으로써, 프레임워크 간의 호환성을 확보한다. ONNX는 다양한 하드웨어 백엔드와도 연결될 수 있어, 향후 NPU 이외의 다양한 엣지나 서버 환경에서도 실행가능성을 확보할 수 있다. 이후 ONNX 모델을 Rknn toolkit을 통해 변환함으로써 Rockchip 기반 NPU에서 직접 실행 가능한 모델로 최종 변환한다.|

<br>
 
# Project-Design & Implementation
| 항목 | 내용 |
|:---  |---  |
| (1) 요구사항 정의 |&nbsp; __1. Overview__ <br> 전체 연구가 진행되는 플로우차트는 아래와 같다.<br> ![Flowchart](https://github.com/ei-ai/eiai/blob/main/flowchart.png)<br> &nbsp; __2. Hardware aware NAS__ <br/> 해당 기능을 달성하기 위한 프레임워크로써 선행연구들에서 제시한 방식을 따른다.<br>선행연구 프레임워크 구조<sub>[1](#[1])</sup> <br/> ![HAT_Framework](https://github.com/ei-ai/eiai/blob/main/HAT_overview.jpg)|
| (2) 전체 시스템 구성 | 다음은 본 연구에서 사용하고자 하는 NAS 알고리즘을 도식화한 것이다.<sup>[2](#[2])</sup> <br/> ![NAS_pipeline](https://github.com/ei-ai/eiai/blob/main/NAS_pipeline.png) <br>   <br><br> &nbsp; __1.	Search space__ <br> Search space는 알고리즘이 탐색을 수행하는 공간을 의미한다. NAS는 이 공간 내에서 최적의 모델 아키텍처를 탐색한다. 탐색 공간은 탐색할 수 있는 모든 모델 구조들의 집합으로, 어떤 연산을 쓸지, 연산들이 어떻게 연결될지 등이 정의되어 있다. 본 연구에서는 다음과 같은 구성 요소를 포함한 탐색 공간을 정의한다: <br> __-	Arbitrary Encoder-Decoder Attention__ <br> 모든 encoder layer 를 decoder가 자유롭게 참조할 수 있도록 한다. <br> __-	Heterogeneous Transformer Layers__ <br> 각 layer 별로 hidden size, head 수, FFN 크기 등이 다르게 구성될 수 있어, 다양한 하드웨어 특성에 맞춘 유연한 아키텍처 설계가 가능하다. <br> __-	Hyperparameter (QKV Dimension)__ <br> 각 하드웨어에 최적화된 qkv dim을 조정하도록 구현하면서 메모리 사이즈를 고려한 트랜스포머의 탐색이 가능하도록 한다.<br><br> &nbsp; __2.	Search Strategy__ <br> Search Strategy는 탐색 방법으로 Search space 내의 어떤 아키텍처를 선택할지 결정하는 전략이다. 탐색 방법으로는 강화학습, 진화 알고리즘, 그래디언트 방식 등 여러 알고리즘이 쓰인다. 본 연구에서는 Supertransformer라는 모든 Subtransformer들을 포함하는 큰 네트워크를 한 번 훈련시키고, 진화 알고리즘(Evolutionary Search)를 통해 하드웨어 latency가 가장 낮고 성능이 좋은 Subtransformer를 탐색한다. 또한, HPO(Hyperparameter Optimization)을 통해 하드웨어에 최적화된 qkv dim 구성을 선택할 수 있도록 한다.<br><br> &nbsp; __3.	Performance Estimation Strategy__ <br> Performance Estimation Strategy는 선택된 아키텍처의 성능을 평가하거나 추정하는 방법이다. 보통 NAS는 search strategy가 하나 혹은 여러 개의 후보 아키텍처를 추출하고, 이들의 성능을 예측한다. 본 연구에서는 다음과 같은 성능 추정 방법을 채택한다: <br> __- weight sharing__ <br> subtransformer의 개별 학습 없이 이미 학습된 supertransformer의 weight를 사용하여 빠른 성능 평가가 가능한다. <br> __-	하드웨어 기반 Latency 측정__ <br> FLOPs 대신 실제 하드웨어에서의 지연시간을 측정하여 플랫폼 맞춤 성능 평가가 가능하다. <br><br>✅ __실험 환경 구성__ <br> - 하드웨어: ROCK Pi 5B (Rockchip RK3588 ARM SoC 기반 NPU 보드)<br> - 운영체제 (OS): Ubuntu 22.04 (aarch64), Debian (Radxa Rock 5B 이미지)<br> - 모델 아키텍처: Transformer 기반 구조 <br> - 사용 데이터셋: WMT’14 En-De, WMT’14 En-Fr, WMT’19 En-De, IWSLT’14 De-En<br> <details> <summary>requirements.txt</summary> rknn-toolkit2==2.3.0 <br> rknn-toolkit-lite2==1.5.0   <br> cffi==1.17.1 <br> click==8.1.8 <br> clorama==0.4.6 <br> coloredlogs==15.0.1 <br> ConfigArgParse==1.7 <br> Cython==3.0.11 <br> fastBPE==0.1.1 <br> filelock==3.16.1 <br> flatbuffers==24.12.23 <br> fsspec==2024.12.0 <br> humanfriendly==10.0 <br> Jinja2==3.1.5 <br> networkx==3.2.1 <br> numpy==2.0.2 <br> nvidia-cublas-cu12==12.4.5.8 <br> nvidia-cuda-cupti-cu12==12.4.127 <br> nvidia-cuda-nvrtc-cu12==12.4.127 <br> nvidia-cuda-runtime-cu12==12.4.127 <br> nvidia-cudnn-cu12==9.1.0.70 <br> nvidia-cufft-cu12==11.2.1.3 <br> nvidia-curand-cu12==10.3.5.147 <br> nvidia-cusolver-cu12==11.6.1.9 <br> nvidia-cusparse-cu12==12.3.1.170 <br> nvidia-nccl-cu12==2.21.5 <br> nvidia-nvjitlink-cu12==12.4.127 <br> nvidia-nvtx-cu12==12.4.127 <br> onnx==1.17.0 <br> onnxruntime==1.19.2 <br> packaging==24.2 <br> portalocker==3.1.1 <br> protobuf==5.29.2 <br> pycparser==2.22 <br> regex==2024.11.6 <br> sacrebleu==2.5.0 <br> sacremoses==0.1.1 <br> sympy==1.13.1 <br> tabulate==0.9.0 <br> tensorboardX==2.6.2.2 <br> torch==2.5.1 <br> tqdm==4.67.1 <br> triton==3.1.0 <br> typing_extensions==4.12.2 <br> ujson==5.10.0</details>|
| (3) 주요엔진 및 기능 설계 |  **전체 프레임워크:** <br><br> 1. 모든 SubTransformer들의 조합을 포함한 거대한 모델인 SuperTransformer를 학습시킨다. <br> 2. 특정 하드웨어 플랫폼에 대해 (SubTransformer 아키텍처, 측정된 latency) 쌍으로 구성된 데이터셋을 구축한다.<br> 3. 위의 데이터셋을 통해 latency predictor를 학습시킨다. <br> 4. 학습된 latency predictor를 사용하여 하드웨어의 Latency 제약 조건 하에서 evolutionary search를 수행하고, 해당 하드웨어에 최적화된 SubTransformer 모델을 탐색한다.<br> 5. 최종 선발된 SubTransformer 모델을 다시 학습하여 최종 성능을 도출한다. <br><br><hr> **상세 구현 내용:** <br><br>&nbsp; __1. Search Space__ <br>**- Supertransformer 구성:** search space에서 가장 큰 모델로 정의되며, 학습을 통해 다양한 Subtransformer 구조를 랜덤으로 샘플링한다.<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __* Arbitrary Encoder-Decoder Attention:__ 디코더가 인코더의 마지막 레이어만 참고하는 기존 Transformer구조와 달리 디코더의 각 레이어가 여러 인코더의 레이어를 선택적으로 참고할 수 있도록 구현한다. 이는 여러 인코더 레이어의 key와 value를 합쳐 디코더에 전달하므로 정확도 향상 효과를 기대할 수 있으며, 추가적인 파라미터나 latency 증가가 거의 없다.<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__* Heterogeneous Transformer Layers__: Transformer의 인코더, 디코더 레이어 설정을 유동적으로 바꿀 수 있도록 하는 방식이다. Embedding Dim, Hidden Dim in FFN, Head Num, 레이어 개수를 다르게 설정할 수 있어, 필요에 따라 모델의 깊이나 무게 조절이 가능하다.<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__* Hyperparameter (QKV Dimension)__: Q, K, V 차원을 Search space에 포함시켜 attention 연산의 메모리 사용량과 연산량을 동적으로 조절할 수 있도록 한다. 이를 통해 하드웨어 플랫폼별 메모리 제약 하에서도 최적화된 Subtransformer 구조의 탐색이 가능해지며, 성능 효율을 극대화할 수 있다. <br><br>&nbsp; __2. Search Strategy__ <br> __- Evolutionary Search__: 학습된 latency predictor를 사용하여, 다양한 Subtranformer 후보 아키텍처의 latency를 추정한다. 여기에서 하드웨어의 latency constraint를 만족하고, validation loss가 가장 낮은 아케텍처를 선택하면서 탐색을 진행한다. <br> __- HPO (Hyperparameter Optimization)__ : Search space 내 하이퍼파라미터(qkv dimension)에 대해 HPO 기법을 활용하여 성능과 latency를 동시에 만족시키는 최적 조합을 탐색한다. <br><br> &nbsp;__3. Performance Estimation Strategy__ <br> __- Weight Sharing__: Supertransformer의 학습 시 다양한 구조의 Subtransformer를 무작위로 샘플링한다. 이 과정에서 각 SubTransformer 가 임베딩, FFN, Attention, 레이어 수 등 다양한 구조의 가중치를 부분적으로 업데이트하면서 공유하도록 한다. 이 구조는 수많은 Subtransformer 아키텍처가 Supertransformer의 공통 weight를 재사용하여 빠른 연산과 효율적인 학습이 구현될 수 있다. <br> __- Latency Predictor 기반 예측__: 탐색 수행 전 하드웨어 플랫폼에서 다양한 Subtransformer 구조와 하드웨어에서의 latency를 측정해 (아키텍처, latency)쌍의 데이터셋을 구성하고, 이를 학습하여 latency predictor를 구축한다. 이 모델은 입력된 Subtransformer 아키텍처 구조에 대해 실제 latency와 매우 근접한 예측 latency 값을 출력하도록 한다.<br><br><hr>__실험 내용 및 결과 요약:__ <br>아래는 전반적인 실험 내용과 결과를 정리한 것으로 자세한 내용은 __*(4) 주요 기능의 구현*__  에서 확인할 수 있다. <br><br> __- SuperTransformer 학습:__ 설정된 design space에서 embedding dim, FFN dim, head 수, layer 수 등을 조합해 트랜스포머 구조를 랜덤 샘플링하고 학습하였다. 이를 통해, 다양한 구조에 대한 효율적인 학습이 가능하고, 추후 성능 추정에 활용할 수 있는 상위모델을 생성할 수 있다. <br> __- Latency Predictor 학습:__ (SubTransformer 구조, 실측 latency) 쌍을 데이터셋으로 사용하여 학습하였다. 실제 latency와 거의 일치하는 정확도를 보이며, 실측 없이 예측 latency를 사용하여 학습할 수 있도록 한다.<br> __- Evolutionary Search:__ latency predictor로 예측 latency를 추출하고, validation loss로 성능 평가하여 Subtransformer를 최종 선택한다. 이를 통해, random search보다 낮은 validation loss를 보이는 구조를 탐색함을 알 수 있다. <br> __- 최종 SubTransformer 학습 및 평가__: 선택된 구조를 초기화하여 학습 후 latency 측정 및 BLEU score 계산한다.<br><br> |
| (4) 주요 기능의 구현 | __주요 SW 모듈의 구현__ <br><br> &nbsp;__1.	Evolutionary Search__ <br>본 연구에서는 최적의 Subtransformer 구조를 하드웨어 latency 제약을 고려해 자동으로 탐색한다. 이때 Evolutionary Search 기반의 탐색 전략이 사용된다. Evolutionary Search는 다음과 같은 프레임워크 순서를 따른다.<br> __-	초기 population 생성__: 탐색 공간에서 기반해 latency constraint를 만족하는 Subtransformer 후보를 랜덤하게 생성한다. 이것이 초기 Population이 된다. <br> __-	Latency predictor를 통한 예측 latency 계산__: 각 후보 아키텍처는 훈련된 latency predictor를 통해 예측 latency가 계산된다.<br> __-	Validation Loss 기반 성능 평가__: 유효한 후보들에 대해서 Supertransformer의 가중치를 공유(weight sharing)받아, validation set을 이용해 validation loss를 측정한다. 이를 통해 추가 학습 없이 성능 근사치를 얻을 수 있다. <br> __- Selection__: Validation loss가 가장 낮은 후보들을 parent 개체로 선택하고, 이 parent 개체들은 다음 iteration의 기반이 된다.<br> __- Crossover과 Mutation__: 두 개체의 파라미터를 조합해 새로운 구조를 생성하거나(crossover), 특정 구조의 일부를 랜덤하게 변화시켜 새로운 구조를 생성한다(mutation). 변경된 후보가 latency constraint를 만족할 때 이를 새로운 후보로 채택하여 다음 iteration에서 사용한다.<br> __-	Iteration__: 다음 세대의 population은 parent + crossover + mutation 로 구성된다. 이 population을 기반으로 정의된 횟수만큼 iteration을 거치며 점점 validation loss가 낮고 latency 제약을 만족하는 최적의 Subtransformer 아키텍처를 찾는다.<br><br>&nbsp; __2.	weight sharing__ <br> 본 연구에서는 탐색 비용을 최소화하기 위해, 단일 SuperTransformer 모델 내에서 모든 SubTransformer가 가중치를 공유하는 방식을 채택한다. SuperTransformer는 embedding 차원, FFN hidden 차원, attention head 수, 레이어 수 등 다양한 아키텍처 구성 요소에 대한 최대 범위의 파라미터를 포함하는 상위 모델로, 다양한 구조의 SubTransformer를 내포할 수 있도록 설계된다. 학습 단계에서는 SuperTransformer를 통해 weight를 공유하며 학습하고, 탐색 단계에서 샘플링된 Subtransformer 아키텍처에 맞게 Supertransformer의 weight 중 필요한 부분만 슬라이싱하여 사용한다. 이로 인해 각 Subtransformer는 별도의 weight table의 생성이나, 추가적인 학습 없이 공통된 weight를 기반으로 효율적인 연산이 가능해진다. <br>아래는 각 모듈별로 적용된 weight sharing 방식에 대한 설명이다:<br><br> __-	Embedding layer__ <br> SubTransformer 구조에 따라 encoder 또는 decoder의 embedding 차원을 설정하고, SuperTransformer가 보유한 embedding weight 중 해당 차원만큼 앞부분을 슬라이싱하여 사용한다. 이 방식은 수많은 구조가 서로 다른 embedding 차원을 사용하더라도 하나의 embedding weight를 공유할 수 있게 한다. <br> __-	Linear layer__ <br> SuperTransformer는 최대 입력/출력 차원에 해당하는 전체 weight를 정의하고, SubTransformer의 구조에 따라 입력 차원 기준으로 column을, 출력 차원 기준으로 row를 잘라 weight를 슬라이싱한다. 이로써 각 SubTransformer는 다양한 FFN이나 attention 구성에서도 공통 weight의 일부만을 재사용하게 된다.<br> __-	Multi-head Attention (QKV Weight)__ <br>SuperTransformer는 Q, K, V에 해당하는 weight를 하나의 큰 in-proj_weight로 정의하며, SubTransformer는 구조에 따라 필요한 부분만 슬라이싱하여 사용한다. Q, K, V는 start:end 범위로 각각 나뉘며, 각 파라미터는 SubTransformer의 입력 차원(sample_dim)만큼 앞부분만 사용된다. 이 방식은 다양한 head 수, qkv 차원, embed dim을 갖는 구조에서도 하나의 QKV weight를 효율적으로 공유할 수 있게 한다.<br><br><hr> __실험 전체 개요__ <br> <br> **1. 실험 목적** <br> 하드웨어 환경에서의 latency 및 메모리 제약 조건 하에서도 높은 성능을 유지할 수 있는 Transformer 아키텍처를 자동으로 탐색할 수 있는 NAS 기반 프레임워크의 유효성을 검증한다.<br><br> **2.	실험 환경 및 설정** <br> - 데이터셋: WMT'14 En-De, WMT'14 En-Fr, WMT'19 En-De, IWSLT'14 De-En <br> - 하드웨어 플랫폼: ROCK Pi 5B (Rockchip RK3588 ARM SoC 기반 NPU 보드), NVIDIA RTX 3070 GPU <br> -	평가 지표: BLEU score, Validation loss, Latency <br><br> **3. 구성 기능별 실험** <br> **3.1 Supertransformer 학습** <br> - 다양한 SubTransformer 구조를 weight sharing 기반으로 학습할 수 있는 상위 모델 SuperTransformer를 사전 학습한다. <br>**- 방법 및 설계** <br> <details> <summary> Design space(WMT 기준)</summary> : Supertransformer search space로, 수많은 Subtransformer를 샘플링하는 공간 <br> - Embedding dim: [512, 640]<br>- FFN hidden dim: [1024, 2048, 3072]<br>- Head 수: [4, 8]<br>- Decoder layer 수: [1, 2, 3, 4, 5, 6]<br> - encoder-decoder attention: 각 디코더 레이어가 1에서 3개의 인코더 레이러를 참고하도록 설정 </details> &nbsp;&nbsp; &nbsp;* QKV dim은 512 또는 64를 사용하여 설정하여 메모리 사용량 제한 고려 <br> &nbsp;&nbsp;&nbsp; * IWSLT 데이터셋에서는 더 작은 구조 사용 <br>**- 실험 결과:** Design space 내에서 다양한 subtransformer 구조를 샘플링하여 Supertransformer를 학습하여, weight sharing 기반으로 효율적인 학습이 가능함을 확인하였다. <br><br> **3.2 Latency Predictor** <br> - SubTransformer의 latency를 실측 없이 예측할 수 있는 모델을 구축하여 탐색 효율성을 향상시킨다. <br> **- 방법 및 설계** <br> &nbsp;&nbsp;&nbsp; * 입력: subtransformer 아키텍처를 표현하는 10개 feature vector <br> &nbsp;&nbsp;&nbsp; * 학습 데이터: 하드웨어에서 측정된 (Subtranformer구조, 실측 latency) 쌍 수집 <br>  &nbsp;&nbsp;&nbsp; * Latency Predictor 구성: 3-layer MLP <br> **- 실험 결과:** RMSE 0.1초 수준의 높은 예측 정확도를 달성하고, 실제 latency와 거의 일치하는 성능을 보인다. <br><br> **3.3 Evolutionary Search** <br> - 하드웨어의 latency constraint를 만족하며 성능이 우수한 Subtransformer를 자동 탐색한다. <br> **- 방법 및 설계** <br> &nbsp;&nbsp; &nbsp;* latency predictor를 사용해 예측 latency 계산 <br> &nbsp;&nbsp; &nbsp;* validation loss기반 selection <br> &nbsp;&nbsp; &nbsp;* population=125, parent=25, mutation=50, crossover=50으로 구성해 30회 iteration 수행 <br> **- 실험 결과:** Random Search보다 평균적으로 약 0.015~0.03 낮은 validation loss를 가진 SubTransformer를 탐색하며, 성능과 latency 모두를 고려한 최적 구조를 탐색 가능함을 확인하였다. <br><br> **3.4 최종 Supertransformer 학습 및 평가** <br> - 탐색된 SubTransformer를 처음부터 독립적으로 학습하여 BLEU 및 latency를 통해 성능과 효율성 모두 평가한다. <br> **- 방법 및 설계** <br>&nbsp;&nbsp; &nbsp;* 선택된 Subtransformer구조를 초기화 후 독립 학습  <br> &nbsp;&nbsp; &nbsp;* 선택된 모델을 이용해 번역 작업을 수행하고, latency를 측정 <br> &nbsp;&nbsp; &nbsp;* 번역 결과와 reference간의 정합도를 평가하여 BLEU score를 계산 <br> **- 진행 상황:** 현재 Subtransformer의 학습은 완료된 상태이고, BLEU score와 latency 측정 중이며, 성능-속도 trade-off 평가 결과를 도출할 예정이다.<br><br> |
| (5) 기타 | __참고문헌:__ <br> - <a name="[1]">1</a>:[Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, Song Han, "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing," arXiv preprint arXiv:2005.14187, 2020.](https://arxiv.org/abs/2005.14187)  <br> - <a name="[2]">2</a>: [Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, "Neural Architecture Search: A Survey," arXiv preprint arXiv:1808.05377, 2019.](https://arxiv.org/abs/1808.05377)|

<br>

<!-- Template for PROJECT REPORT of CapstoneDesign 2025-2H, initially written by khyoo -->
<!-- 본 파일은 2025년도 컴공 졸업프로젝트의 <1차보고서> 작성을 위한 기본 양식입니다. -->
<!-- 아래에 "*"..."*" 표시는 italic체로 출력하기 위해서 사용한 것입니다. -->
<!-- "내용"에 해당하는 부분을 지우고, 여러분 과제의 내용을 작성해 주세요. -->
<!-- 이미지는 절대경로로 기입 -->

# Team-Info
| (1) 과제명 | *본인 팀에서 수행중인 프로젝트의 이름을 작성합니다*
|:---  |---  |
| (2) 팀 번호 / 팀 이름 | *팀ID-팀이름* |
| (3) 팀 구성원 | 홍길동 (0000000): 리더, *맡은 역할* <br> 김팀원 (0000000): 팀원, *맡은 역할* <br> 이멤버 (0000000) : 팀원, *맡은 역할*			 |
| (4) 팀 지도교수 | OOO 교수님 |
| (5) 과제 분류 | 연구 과제 |
| (6) 과제 키워드 | Hardware aware NAS, HPO, Efficient Transformer |
| (7) 과제 내용 요약 | Transformer 모델은 NLP 분야에서 널리 사용되고 있지만, 모델의 크기와 연산량이 증가하면서 높은 컴퓨팅 자원을 요구하게 되었다. 특히 엣지 디바이스와 같은 제한된 환경에서는 모델의 추론에서의 지연과 메모리 사용에서의 제약과 같은 문제가 발생한다. 이러한 문제를 해결하기 위해, 하드웨어 제약을 고려한 경량화 모델 탐색이 필요하다. 본 논문은 NAS(Neural Architecture Search)와 HPO(Hyperparameter Optimization)를 결합한 기법을 통해 이를 해결하고자 한다. 즉, NAS를 통해 구조를 탐색하고, HPO를 통해 성능과 제약 조건 모두를 고려하는 최적의 모델을 찾는다. 특히 NAS의 거대한 서치 스페이스가 초래하는 서치 연산 문제와 메모리 제약조건에 대한 비인지성 문제를 HPO로 보완한다. 이를 통해 주어진 하드웨어에 최적화된 효율적인 트랜스포머 모델을 설계할 수 있다. |

<br>

# Project-Summary
| 항목 | 내용 |
|:---  |---  |
| (1) 문제 정의 | 현재 트랜스포머를 비롯한 딥러닝 기반 NLP 인공지능 기술은 우리 일상 속에 깊이 자리 잡고 있다. 그러나 이러한 서비스 대부분은 데이터 센터에서 수많은 반도체를 활용하는 공급자 중심의 클라우드 기반 AI 형태로 제공되고 있다. 그리고 클라우드 기반 AI는 대표적으로 중앙 데이터 센터와의 송수신 과정에서 발생하는 응답지연, 데이터 센터에 사용되는 막대한 유지비용, 보안에서의 취약성과 같은 문제들을 동반한다. 
이를 해결하기 위한 대안으로 중앙 클라우드에 대한 의존을 줄이는 엣지 AI가 주목받고 있지만, 현재 딥러닝 모델의 크기와 연산량이 지속적으로 증가하면서 엣지 디바이스의 제한적인 환경에서 모델의 활용에 어려움이 생기고 있다.
이에 본 연구는 대규모 NLP 인공지능을 사용하는 기업이나 엣지 디바이스에서 LLM을 사용하고자 하는 일반 유저를 타겟 고객으로 설정하고, 엣지 디바이스가 가지는 연산능력과 메모리 등 하드웨어 환경적 제약을 고려한 모델 최적화를 실현하고자 한다.|
| (2) 기존연구와의 비교 | 
1. 전통적인 모델의 최적화
기존의 전통적인 모델 최적화는 전문가의 직관과 경험을 기반으로 하이퍼파라미터나 모델 구조를 수동으로 조정해 성능을 점진적으로 향상시키는 방식이다. 이 방식은 설계자 중심의 직관적 이해와 수정에 용이하다는 장점이 있지만, 트랜스포머와 같은 복잡한 모델에는 적용이 어렵고, 시간과 비용이 많이 드는 단점이 있다. 이를 극복하기 위해 NAS(Neural Architecture Search) 기법이 등장하였고, 다양한 데이터셋과 목적에 맞춰 모델을 자동으로 설계할 수 있게 되었다. 
2. NAS (Neural Architecture Search)
NAS는 설계자의 개입 없이 알고리즘적으로 모델 구조를 탐색하며, 복잡한 모델의 설계에서 인간의 직관과 경험에 대한 의존도를 줄여준다. 그러나, 기존의 NAS는 기존 NAS는 주로 FLOPs에 기반한 latency 측정에 의존하고 있으며, 오직 서치 속도나 모델의 정확도에만 집중해왔다. FLOPs에 의존하지 않는 연구들은 주로 동질적인 하드웨어 환경에서만 실험을 거쳐 다양한 하드웨어 환경(예: NPU, FPGA)의 특성을 반영하지 못한다는 한계가 있다. 이는 NAS에서 탐색한 모델의 성능이 실제 하드웨어에서의 성능과 괴리가 생기게 만든다.
3. HAT (Hardware-aware Transformer)
HAT는 하드웨어 환경(CPU, GPU)을 고려하여 트랜스포머 모델을 자동 설계하는 NAS 기반 기법을 제안한 연구로, 기존 NAS에 비해 실제 하드웨어에 맞는 모델을 설계한다는 데 의의가 있다. 그러나 (1) 메모리 제약을 고려하지 않아 latency를 중점적으로 탐색한 결과가 하드웨어 제약 전반을 반영하지 못했고, (2) NPU 환경에서는 적용이 불가능하다는 한계가 있다.
4.	본 연구의 차별점 및 강점
본 연구는 기존의 한계를 극복하기 위해 다음과 같은 차별점을 가진다:
- NAS를 사용해 하드웨어 친화적인 트랜스포머 모델 구조를 자동으로 탐색한다.
-	FLOPs가 아닌 실제 하드웨어에서의 인퍼런스 실행을 통해 latency를 직접 측정한다.
-	기존 NAS/HAT 연구들이 무시했던 메모리 제약조건을 서치 스페이스에 포함하여 엣지 디바이스 환경에 적합한 실질적 최적화를 실현한다.
-	CPU, GPU뿐 아니라 NPU 환경을 고려한 연구를 진행한다.|
| (3) 제안 내용 | **주요 채점내용* **본 프로젝트에서 제시한 문제를 해결하기 위해 새롭제 제안하는 해결책 or 해결책들에 대하여 기술 .* |
| (4) 기대효과 및 의의 | *프로젝트의 결과물을 통하여 얻을 수 있는 기대효과 및 의의에 대하여 기술 .* |
| (5) 주요 기능 리스트 | **주요 채점내용** *(3)에서 제안한 해결책들을 지원or구현하기 위하여 필요한 주요 기능 혹은 기능을을 List-up하고, <br> 각각에 대하여 설명* <br> * 본 항목의 내용을 충실히 기재 바람니다. *|

<br>
 
# Project-Design & Implementation
| 항목 | 내용 |
|:---  |---  |
| (1) 요구사항 정의 | *프로젝트를 완성하기 위해 필요한 요구사항을 설명하기에 가장 적합한 방법을 선택하여 기술* <br> 예) <br> - 기능별 상세 요구사항(또는 유스케이스) <br> - 설계 모델(클래스 다이어그램, 클래스 및 모듈 명세서) <br> - UI 분석/설계 모델 <br> - E-R 다이어그램/DB 설계 모델(테이블 구조) |
| (2) 전체 시스템 구성 | *프로젝트를 위하여, SW 전체 시스템의 구조를 보인다. (가능하다면, 사용자도 포함) <br> 주요 SW 모듈을 보이고, 각각의 역할을 기술한다. <br>만약, 오픈소스 혹은 외부 모듈을 사용한다면 이또한 기술한다.* |
| (3) 주요엔진 및 기능 설계 | *프로젝트의 주요 기능 혹은 모듈의 설계내용에 대하여 기술한다 <br> SW 구조 그림에 있는 각 Module의 상세 구현내용을 자세히 기술한다.* |
| (4) 주요 기능의 구현 | *<주요기능리스트>에 정의된 기능 중 최소 2개 이상에 대한 상세 구현내용을 기술한다.* |
| (5) 기타 | *기타 사항을 기술*  |

<br>
